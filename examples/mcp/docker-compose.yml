services:
  inference-gateway:
    image: ghcr.io/inference-gateway/inference-gateway:latest
    pull_policy: always
    container_name: inference-gateway
    environment:
      DEEPSEEK_API_KEY: ${DEEPSEEK_API_KEY:-}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY:-}
      CLOUDFLARE_API_KEY: ${CLOUDFLARE_API_KEY}
      COHERE_API_KEY: ${COHERE_API_KEY}
      GROQ_API_KEY: ${GROQ_API_KEY}
      OLLAMA_CLOUD_API_KEY: ${OLLAMA_CLOUD_API_KEY}
      GOOGLE_API_KEY: ${OLLAMA_CLOUD_API_KEY}
      MISTRAL_API_KEY: ${MISTRAL_API_KEY}
    restart: unless-stopped
    networks:
      - mcp-demo

  mcp-demo-server:
    build:
      context: ./mcp-server
      dockerfile: Dockerfile
    container_name: mcp-demo-server
    environment:
      DEMO_MESSAGE: Hello from MCP Demo Server!
      ENVIRONMENT: development
    restart: unless-stopped
    healthcheck:
      test:
        - CMD
        - sh
        - -c
        - 'curl -f -X POST http://localhost:3000/mcp -H "Content-Type: application/json" -d "{\"jsonrpc\":\"2.0\",\"method\":\"ping\",\"id\":1}" || exit 1'
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    networks:
      - mcp-demo

  context7:
    image: mekayelanik/context7-mcp:stable
    container_name: context7
    environment:
      PORT: "8010"
      NODE_ENV: production
      PROTOCOL: SHTTP
    restart: unless-stopped
    healthcheck:
      test:
        - CMD
        - sh
        - -c
        - 'wget --spider -q http://localhost:8010/healthz || exit 1'
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    networks:
      - mcp-demo

  cli:
    profiles:
      - cli
    image: ghcr.io/inference-gateway/cli:latest
    environment:
      # MCP configuration
      INFER_MCP_ENABLED: true
      INFER_MCP_CONNECTION_TIMEOUT: "30"
      INFER_MCP_DISCOVERY_TIMEOUT: "30"
    volumes:
      # Mount MCP config
      - ./mcp-config.yaml:/home/infer/.infer/mcp.yaml:ro
      # Persist conversation data
      - infer-data:/home/infer/.infer/data
    command:
      - chat
    depends_on:
      inference-gateway:
        condition: service_started
      mcp-demo-server:
        condition: service_healthy
      context7:
        condition: service_healthy
    networks:
      - mcp-demo

networks:
  mcp-demo:
    driver: bridge

volumes:
  infer-data:
